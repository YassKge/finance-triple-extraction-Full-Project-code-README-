{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxd9TcDxF6ynQNaYpbiyrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YassKge/finance-triple-extraction-Full-Project-code-README-/blob/main/finance_triple_extraction_Full_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOM3Egkcf08_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzrkWtF-dHOr",
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjFenXc3dXE3",
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883",
        "id": "gTIHAM2igIoz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b",
        "id": "x4w4UF5ggIo5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883",
        "id": "yLUOciTDgJN1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b",
        "id": "ZjBW_EzngJN5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883",
        "id": "LCxa4TfWgKvD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b",
        "id": "9elUreMqgKvH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883",
        "id": "9gVwmfTFgLQT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b",
        "id": "tJwGB3e4gLQW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7eb665-9ef0-4eea-ff49-33d032d82883",
        "id": "A6zegO3QgNHU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 19482 financial triples successfully.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class FinancialTripleExtractor:\n",
        "    \"\"\"\n",
        "    A comprehensive system for extracting knowledge triples from financial texts.\n",
        "    Handles JSON data input and produces structured triples in the format:\n",
        "    (Subject, Relation, Object)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        # Try to load spaCy models in order of preference\n",
        "        self.nlp = self._load_spacy_model()\n",
        "\n",
        "        # Financial relation keywords\n",
        "        self.financial_verbs = {\n",
        "            \"incur\": \"incurred\",\n",
        "            \"use\": \"used\",\n",
        "            \"allocate\": \"allocated_to\",\n",
        "            \"report\": \"reported\",\n",
        "            \"generate\": \"generated\",\n",
        "            \"earn\": \"earned\",\n",
        "            \"pay\": \"paid\",\n",
        "            \"receive\": \"received\",\n",
        "            \"invest\": \"invested\",\n",
        "            \"spend\": \"spent\",\n",
        "            \"record\": \"recorded\",\n",
        "            \"recognize\": \"recognized\",\n",
        "            \"distribute\": \"distributed\",\n",
        "            \"issue\": \"issued\",\n",
        "            \"repurchase\": \"repurchased\",\n",
        "            \"acquire\": \"acquired\",\n",
        "            \"sell\": \"sold\",\n",
        "            \"increase\": \"increased\",\n",
        "            \"decrease\": \"decreased\",\n",
        "            \"maintain\": \"maintained\",\n",
        "            \"hold\": \"held\"\n",
        "        }\n",
        "\n",
        "        # Financial entity patterns\n",
        "        self.money_pattern = re.compile(r'\\$[\\d,]+\\.?\\d*\\s?(?:million|billion|thousand|M|B|K)?', re.IGNORECASE)\n",
        "        self.percentage_pattern = re.compile(r'\\d+\\.?\\d*\\s?%')\n",
        "        self.date_pattern = re.compile(r'(?:December|January|February|March|April|May|June|July|August|September|October|November)\\s+\\d{1,2},?\\s+\\d{4}')\n",
        "        self.year_pattern = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Try to load spaCy models in order of preference.\"\"\"\n",
        "        models_to_try = [\n",
        "            \"en_core_web_trf\",\n",
        "            \"en_core_web_lg\",\n",
        "            \"en_core_web_md\",\n",
        "            \"en_core_web_sm\"\n",
        "        ]\n",
        "\n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                nlp = spacy.load(model_name)\n",
        "                if self.verbose:\n",
        "                    print(f\"âœ… Successfully loaded {model_name}\")\n",
        "                return nlp\n",
        "            except (OSError, ValueError):\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Could not load {model_name}\")\n",
        "                continue\n",
        "\n",
        "        # If all models fail, create a basic pipeline\n",
        "        if self.verbose:\n",
        "            print(\"âš ï¸  No pre-trained models available. Creating basic pipeline...\")\n",
        "        try:\n",
        "            nlp = spacy.blank(\"en\")\n",
        "            if self.verbose:\n",
        "                print(\"âœ… Created basic spaCy pipeline\")\n",
        "            return nlp\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âŒ Failed to create any spaCy pipeline: {e}\")\n",
        "                print(\"\\nðŸ”§ SOLUTION: Run these commands:\")\n",
        "                print(\"!pip install -U spacy\")\n",
        "                print(\"!python -m spacy download en_core_web_sm\")\n",
        "            raise Exception(\"Cannot initialize spaCy. Please install spaCy models.\")\n",
        "\n",
        "    def load_json_data(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load JSON data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text data from file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def process_text_segments(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Process text that's divided by --- separators.\"\"\"\n",
        "        # Split by --- separator\n",
        "        segments = [segment.strip() for segment in text.split('---') if segment.strip()]\n",
        "\n",
        "        results = []\n",
        "        for i, segment in enumerate(segments):\n",
        "            if self.verbose:\n",
        "                print(f\"ðŸ“„ Processing segment {i+1}/{len(segments)}...\")\n",
        "            try:\n",
        "                triples = self.extract_financial_triples(segment)\n",
        "                for triple in triples:\n",
        "                    results.append({\n",
        "                        'segment_id': i + 1,\n",
        "                        'subject': triple[0],\n",
        "                        'relation': triple[1],\n",
        "                        'object': triple[2],\n",
        "                        'source_text': segment[:200] + \"...\" if len(segment) > 200 else segment\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print(f\"âš ï¸  Error in segment {i+1}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_entities_regex(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using regex patterns (fallback method).\"\"\"\n",
        "        entities = {\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'YEAR': [],\n",
        "            'ORG': []\n",
        "        }\n",
        "\n",
        "        # Money patterns\n",
        "        money_matches = self.money_pattern.findall(text)\n",
        "        entities['MONEY'] = list(set(money_matches))\n",
        "\n",
        "        # Percentage patterns\n",
        "        percent_matches = self.percentage_pattern.findall(text)\n",
        "        entities['PERCENT'] = list(set(percent_matches))\n",
        "\n",
        "        # Date patterns\n",
        "        date_matches = self.date_pattern.findall(text)\n",
        "        entities['DATE'] = list(set(date_matches))\n",
        "\n",
        "        # Year patterns\n",
        "        year_matches = self.year_pattern.findall(text)\n",
        "        entities['YEAR'] = [match[0] + match[1] for match in year_matches]\n",
        "        entities['YEAR'] = list(set(entities['YEAR']))\n",
        "\n",
        "        # Common organization terms\n",
        "        org_patterns = [\n",
        "            r'\\b(Company|Corporation|Corp|Inc|LLC|Ltd|Group|Holdings|Enterprises)\\b',\n",
        "            r'\\b([A-Z][a-z]+ (?:Company|Corporation|Corp|Inc|LLC|Ltd))\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            org_matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if isinstance(org_matches[0], tuple) if org_matches else False:\n",
        "                entities['ORG'].extend([match[0] for match in org_matches])\n",
        "            else:\n",
        "                entities['ORG'].extend(org_matches)\n",
        "\n",
        "        entities['ORG'] = list(set(entities['ORG']))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract entities using spaCy NER and custom patterns.\"\"\"\n",
        "        entities = {\n",
        "            'PERSON': [],\n",
        "            'ORG': [],\n",
        "            'MONEY': [],\n",
        "            'DATE': [],\n",
        "            'PERCENT': [],\n",
        "            'CARDINAL': [],\n",
        "            'GPE': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try spaCy NER if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and any(pipe in ['ner', 'transformer'] for pipe in self.nlp.pipe_names):\n",
        "                doc = self.nlp(text)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ in entities:\n",
        "                        entities[ent.label_].append(ent.text.strip())\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  spaCy NER failed: {e}. Using regex fallback...\")\n",
        "\n",
        "        # Always use regex patterns as backup/supplement\n",
        "        regex_entities = self.extract_entities_regex(text)\n",
        "\n",
        "        # Merge results\n",
        "        for key in ['MONEY', 'DATE', 'PERCENT']:\n",
        "            if key in regex_entities:\n",
        "                entities[key].extend(regex_entities[key])\n",
        "\n",
        "        entities['ORG'].extend(regex_entities.get('ORG', []))\n",
        "        entities['DATE'].extend(regex_entities.get('YEAR', []))\n",
        "\n",
        "        # Remove duplicates\n",
        "        for key in entities:\n",
        "            entities[key] = list(set(entities[key]))\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_relations_pattern_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using enhanced pattern matching.\"\"\"\n",
        "        relations = []\n",
        "\n",
        "        # Enhanced patterns for financial statements\n",
        "        financial_patterns = [\n",
        "            # Net loss/income patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:incurred|reported|had|recorded)\\s+(?:a\\s+)?net\\s+(loss|income|profit)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"incurred_net_loss\" if \"loss\" in match.group(2).lower() else \"reported_net_income\"\n",
        "            },\n",
        "\n",
        "            # Cash usage patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+used\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)\\s+(?:of\\s+)?cash\\s+(?:in\\s+|for\\s+)?operations?',\n",
        "                'relation': lambda match: \"used_cash_in_operations\"\n",
        "            },\n",
        "\n",
        "            # Revenue patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:generated|earned|reported|recorded)\\s+(?:total\\s+)?(?:revenue|revenues|sales)\\s+of\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"generated_revenue\"\n",
        "            },\n",
        "\n",
        "            # Allocation patterns with years\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+allocated\\s+(?:to\\s+.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"allocated_in\"\n",
        "            },\n",
        "\n",
        "            # Earnings patterns\n",
        "            {\n",
        "                'pattern': r'(.*?)\\s+(?:earnings|income)\\s+(?:.*?\\s+)?(?:in\\s+|for\\s+)?(\\d{4}).*?(?:were?|was)\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:million|billion)?)',\n",
        "                'relation': lambda match: \"earnings_in\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for pattern_info in financial_patterns:\n",
        "            pattern = pattern_info['pattern']\n",
        "            relation_func = pattern_info['relation']\n",
        "\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    subject = self._clean_entity(match.group(1))\n",
        "                    relation = relation_func(match)\n",
        "\n",
        "                    if 'allocated_in' in relation or 'earnings_in' in relation:\n",
        "                        # Special handling for year-based relations\n",
        "                        year = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "                        amount = match.group(3) if match.lastindex >= 3 else match.group(2)\n",
        "                        obj = f\"{year} : {amount}\" if year else amount\n",
        "                    else:\n",
        "                        obj = match.group(2) if match.lastindex >= 2 else \"\"\n",
        "\n",
        "                    if subject and relation and obj:\n",
        "                        relations.append((subject, relation, obj))\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        return relations\n",
        "\n",
        "    def extract_relations_rule_based(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract relations using dependency parsing (if available) or pattern matching.\"\"\"\n",
        "        try:\n",
        "            # Try dependency parsing if available\n",
        "            if hasattr(self.nlp, 'pipe_names') and 'parser' in self.nlp.pipe_names:\n",
        "                doc = self.nlp(text)\n",
        "                relations = []\n",
        "\n",
        "                for sent in doc.sents:\n",
        "                    for token in sent:\n",
        "                        if token.lemma_.lower() in self.financial_verbs:\n",
        "                            # Find subject\n",
        "                            subjects = []\n",
        "                            for child in token.lefts:\n",
        "                                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "                                    subjects.append(child.text)\n",
        "\n",
        "                            # Find objects\n",
        "                            objects = []\n",
        "                            for child in token.rights:\n",
        "                                if child.dep_ in (\"dobj\", \"attr\", \"pobj\"):\n",
        "                                    objects.append(child.text)\n",
        "\n",
        "                            # Create relations\n",
        "                            relation = self.financial_verbs[token.lemma_.lower()]\n",
        "                            for subj in subjects:\n",
        "                                for obj in objects:\n",
        "                                    relations.append((subj, relation, obj))\n",
        "\n",
        "                return relations\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"âš ï¸  Dependency parsing failed: {e}. Using pattern matching...\")\n",
        "\n",
        "        # Fallback to pattern-based extraction\n",
        "        return self.extract_relations_pattern_based(text)\n",
        "\n",
        "    def extract_financial_triples(self, text: str) -> List[Tuple[str, str, str]]:\n",
        "        \"\"\"Extract financial triples using combined approach.\"\"\"\n",
        "        entities = self.extract_entities(text)\n",
        "        relations = self.extract_relations_rule_based(text)\n",
        "        triples = []\n",
        "\n",
        "        # Add rule-based/pattern-based relations\n",
        "        triples.extend(relations)\n",
        "\n",
        "        # Additional pattern-based extractions\n",
        "        additional_patterns = [\n",
        "            # Time period patterns\n",
        "            (r'(?:during|for)\\s+the\\s+(?:year|period)\\s+ended\\s+(.*?),?\\s+(\\d{4})',\n",
        "             lambda m: (\"Company\", \"reporting_period\", f\"{m.group(1).strip()}, {m.group(2)}\")),\n",
        "\n",
        "            # Multiple year data patterns\n",
        "            (r'(.*?)\\s+in\\s+(\\d{4}),?\\s+(\\d{4})\\s+and\\s+(\\d{4})\\s+were?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?),?\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)\\s+and\\s+(\\$[\\d,]+\\.?\\d*\\s?(?:billion|million)?)',\n",
        "             lambda m: [\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(2)} : {m.group(5)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(3)} : {m.group(6)}\"),\n",
        "                 (self._clean_entity(m.group(1)), \"amount_in\", f\"{m.group(4)} : {m.group(7)}\")\n",
        "             ])\n",
        "        ]\n",
        "\n",
        "        # Apply additional patterns\n",
        "        for pattern, extractor in additional_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    result = extractor(match)\n",
        "                    if isinstance(result, list):\n",
        "                        triples.extend(result)\n",
        "                    else:\n",
        "                        triples.append(result)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "        # Entity-relation matching for remaining entities\n",
        "        if entities['ORG'] and entities['MONEY']:\n",
        "            for org in entities['ORG']:\n",
        "                for money in entities['MONEY']:\n",
        "                    if entities['DATE'] or entities.get('YEAR', []):\n",
        "                        dates = entities['DATE'] + entities.get('YEAR', [])\n",
        "                        for date in dates[:3]:  # Limit to avoid too many combinations\n",
        "                            triples.append((org, \"financial_amount_on\", f\"{date} : {money}\"))\n",
        "\n",
        "        # Remove duplicates and clean up\n",
        "        unique_triples = []\n",
        "        seen = set()\n",
        "        for triple in triples:\n",
        "            triple_str = str(triple)\n",
        "            if triple_str not in seen and all(str(x).strip() for x in triple):\n",
        "                seen.add(triple_str)\n",
        "                unique_triples.append(triple)\n",
        "\n",
        "        return unique_triples\n",
        "\n",
        "    def _clean_entity(self, entity: str) -> str:\n",
        "        \"\"\"Clean and normalize entity names.\"\"\"\n",
        "        if not entity:\n",
        "            return \"Company\"\n",
        "\n",
        "        entity = entity.strip()\n",
        "        # Remove common prefixes\n",
        "        prefixes = [\"the\", \"The\", \"during\", \"During\", \"for\", \"For\", \"and\", \"And\"]\n",
        "        for prefix in prefixes:\n",
        "            if entity.startswith(prefix + \" \"):\n",
        "                entity = entity[len(prefix + \" \"):]\n",
        "\n",
        "        # Remove trailing punctuation\n",
        "        entity = re.sub(r'[,.:;]+$', '', entity)\n",
        "\n",
        "        return entity.strip() or \"Company\"\n",
        "\n",
        "    def process_json_file(self, json_file_path: str, text_fields: List[str] = None) -> List[Dict]:\n",
        "        \"\"\"Process JSON file and extract triples from specified text fields.\"\"\"\n",
        "        data = self.load_json_data(json_file_path)\n",
        "        results = []\n",
        "\n",
        "        if text_fields is None:\n",
        "            text_fields = ['text', 'content', 'description', 'summary', 'statement', 'narrative', 'notes']\n",
        "\n",
        "        def extract_from_dict(obj, parent_key=\"\"):\n",
        "            triples = []\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    current_key = f\"{parent_key}.{key}\" if parent_key else key\n",
        "                    if isinstance(value, str) and len(value.strip()) > 10:\n",
        "                        # Check if field name suggests it contains text to analyze\n",
        "                        if any(field in key.lower() for field in text_fields) or len(value) > 50:\n",
        "                            extracted_triples = self.extract_financial_triples(value)\n",
        "                            for triple in extracted_triples:\n",
        "                                triples.append({\n",
        "                                    'source_field': current_key,\n",
        "                                    'subject': triple[0],\n",
        "                                    'relation': triple[1],\n",
        "                                    'object': triple[2],\n",
        "                                    'source_text': value[:200] + \"...\" if len(value) > 200 else value\n",
        "                                })\n",
        "                    elif isinstance(value, (dict, list)):\n",
        "                        triples.extend(extract_from_dict(value, current_key))\n",
        "            elif isinstance(obj, list):\n",
        "                for i, item in enumerate(obj):\n",
        "                    current_key = f\"{parent_key}[{i}]\" if parent_key else f\"[{i}]\"\n",
        "                    triples.extend(extract_from_dict(item, current_key))\n",
        "            return triples\n",
        "\n",
        "        return extract_from_dict(data)\n",
        "\n",
        "    def save_triples_to_csv(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to CSV file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(triples)\n",
        "        df.to_csv(output_file, index=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "    def save_triples_to_json(self, triples: List[Dict], output_file: str):\n",
        "        \"\"\"Save extracted triples to JSON file.\"\"\"\n",
        "        if not triples:\n",
        "            if self.verbose:\n",
        "                print(\"No triples to save.\")\n",
        "            return\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(triples, f, indent=2, ensure_ascii=False)\n",
        "        if self.verbose:\n",
        "            print(f\"âœ… {len(triples)} triples saved to {output_file}\")\n",
        "\n",
        "# Example usage and testing\n",
        "def main(verbose=False):\n",
        "    \"\"\"Initialize the extractor with optional verbose output.\"\"\"\n",
        "    try:\n",
        "        extractor = FinancialTripleExtractor(verbose=verbose)\n",
        "        return extractor\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Failed to initialize extractor: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_your_file(extractor, file_path, verbose=False):\n",
        "    \"\"\"Process your specific input file.\"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n=== Processing Your File: {file_path} ===\")\n",
        "\n",
        "    try:\n",
        "        # Load your text file\n",
        "        if file_path.endswith('.json'):\n",
        "            triples = extractor.process_json_file(file_path)\n",
        "        else:\n",
        "            # Load text file\n",
        "            text_content = extractor.load_text_file(file_path)\n",
        "            triples = extractor.process_text_segments(text_content)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Successfully extracted {len(triples)} triples from your file!\")\n",
        "\n",
        "            # Display first few triples as preview\n",
        "            print(\"\\nðŸ” Preview of extracted triples:\")\n",
        "            for i, triple in enumerate(triples[:10], 1):  # Show first 10\n",
        "                if isinstance(triple, dict) and 'segment_id' in triple:\n",
        "                    print(f\"\\n{i}. Segment {triple['segment_id']}:\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "                    print(f\"   Source: {triple['source_text'][:100]}...\")\n",
        "                else:\n",
        "                    print(f\"\\n{i}. Field: {triple.get('source_field', 'N/A')}\")\n",
        "                    print(f\"   Triple: ({triple['subject']}, {triple['relation']}, {triple['object']})\")\n",
        "\n",
        "            if len(triples) > 10:\n",
        "                print(f\"\\n... and {len(triples) - 10} more triples\")\n",
        "\n",
        "        # Save results\n",
        "        if triples:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            csv_file = f'extracted_triples.csv'\n",
        "            #json_file = f'extracted_triples.json'\n",
        "\n",
        "            extractor.save_triples_to_csv(triples, csv_file)\n",
        "            #extractor.save_triples_to_json(triples, json_file)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"\\nâœ… Results saved to {csv_file}\")\n",
        "\n",
        "        return triples\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        if verbose:\n",
        "            print(f\"âŒ File not found: {file_path}\")\n",
        "            print(\"Make sure the file path is correct and the file exists.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        if verbose:\n",
        "            print(f\"âŒ Error processing file: {e}\")\n",
        "        return []\n",
        "\n",
        "# Clean usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor silently\n",
        "    extractor = main(verbose=False)\n",
        "\n",
        "    # Process file silently\n",
        "    if extractor:\n",
        "        triples = process_your_file(extractor, '/content/extracted_text_only.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only1.txt', verbose=False)\n",
        "       #triples = process_your_file(extractor, '/content/extracted_text_only2.txt', verbose=False)\n",
        "\n",
        "        # Only show final results\n",
        "        if triples:\n",
        "            print(f\"Extracted {len(triples)} financial triples successfully.\")\n",
        "        else:\n",
        "            print(\"No triples extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "def extract_columns_from_csv(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Extract subject, relation, and object columns from CSV file\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV file\n",
        "        output_file (str): Path to output CSV file (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with extracted columns\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(input_file)\n",
        "\n",
        "        # Print original columns for verification\n",
        "        print(\"Original columns:\", df.columns.tolist())\n",
        "        print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        required_columns = ['subject', 'relation', 'object']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "            # Try to find columns with similar names (case insensitive)\n",
        "            available_cols = df.columns.tolist()\n",
        "            for missing_col in missing_columns:\n",
        "                for col in available_cols:\n",
        "                    if missing_col.lower() in col.lower():\n",
        "                        print(f\"Found similar column '{col}' for '{missing_col}'\")\n",
        "\n",
        "        # Extract only the required columns\n",
        "        extracted_df = df[required_columns].copy()\n",
        "\n",
        "        # Display first few rows\n",
        "        print(\"\\nExtracted data (first 5 rows):\")\n",
        "        print(extracted_df.head())\n",
        "        print(f\"\\nExtracted shape: {extracted_df.shape}\")\n",
        "\n",
        "        # Save to new CSV file if output_file is specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nExtracted data saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Column {e} not found in the CSV file.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_columns_manual_parsing(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Alternative method using manual CSV parsing for problematic files\n",
        "    \"\"\"\n",
        "    try:\n",
        "        extracted_data = []\n",
        "\n",
        "        with open(input_file, 'r', encoding='utf-8') as file:\n",
        "            # Read first line to get headers\n",
        "            first_line = file.readline().strip()\n",
        "            headers = first_line.split(',')\n",
        "\n",
        "            print(\"Detected headers:\", headers)\n",
        "\n",
        "            # Find indices of required columns\n",
        "            try:\n",
        "                subject_idx = headers.index('subject')\n",
        "                relation_idx = headers.index('relation')\n",
        "                object_idx = headers.index('object')\n",
        "            except ValueError as e:\n",
        "                print(f\"Error finding column indices: {e}\")\n",
        "                print(\"Available headers:\", headers)\n",
        "                return None\n",
        "\n",
        "            # Read data rows\n",
        "            csv_reader = csv.reader(file)\n",
        "            for row in csv_reader:\n",
        "                if len(row) > max(subject_idx, relation_idx, object_idx):\n",
        "                    extracted_row = {\n",
        "                        'subject': row[subject_idx].strip(),\n",
        "                        'relation': row[relation_idx].strip(),\n",
        "                        'object': row[object_idx].strip()\n",
        "                    }\n",
        "                    extracted_data.append(extracted_row)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        extracted_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        print(f\"\\nExtracted {len(extracted_data)} rows\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(extracted_df.head())\n",
        "\n",
        "        # Save if output file specified\n",
        "        if output_file:\n",
        "            extracted_df.to_csv(output_file, index=False)\n",
        "            print(f\"\\nData saved to: {output_file}\")\n",
        "\n",
        "        return extracted_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manual parsing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your actual file path\n",
        "    input_csv = \"/content/extracted_triples.csv\"\n",
        "    output_csv = \"extracted_columns.csv\"\n",
        "\n",
        "    print(\"=== CSV Column Extractor ===\")\n",
        "    print(\"Extracting subject, relation, and object columns...\\n\")\n",
        "\n",
        "    # Try pandas method first\n",
        "    result = extract_columns_from_csv(input_csv, output_csv)\n",
        "\n",
        "    # If pandas fails, try manual parsing\n",
        "    if result is None:\n",
        "        print(\"\\nTrying alternative parsing method...\")\n",
        "        result = extract_columns_manual_parsing(input_csv, output_csv)\n",
        "\n",
        "    if result is not None:\n",
        "        print(\"\\n=== Extraction completed successfully! ===\")\n",
        "        print(f\"Total rows extracted: {len(result)}\")\n",
        "    else:\n",
        "        print(\"\\n=== Extraction failed ===\")\n",
        "\n",
        "# Quick function for immediate use\n",
        "def quick_extract(file_path):\n",
        "    \"\"\"Quick extraction function\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[['subject', 'relation', 'object']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbaaba-cc03-4b32-d09b-7d5b1b1a2a0b",
        "id": "s_Ye2y0EgNHX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CSV Column Extractor ===\n",
            "Extracting subject, relation, and object columns...\n",
            "\n",
            "Original columns: ['segment_id', 'subject', 'relation', 'object', 'source_text']\n",
            "Original shape: (19482, 5)\n",
            "\n",
            "Extracted data (first 5 rows):\n",
            "       subject             relation  \\\n",
            "0      Company             incurred   \n",
            "1      Company             incurred   \n",
            "2      Company     reporting_period   \n",
            "3  The Company  financial_amount_on   \n",
            "4  The Company  financial_amount_on   \n",
            "\n",
            "                                              object  \n",
            "0                                             losses  \n",
            "1                                               loss  \n",
            "2                                  DecemberÂ 31, 2023  \n",
            "3  the year ended DecemberÂ 31, 2023 : $217.6 million  \n",
            "4                                20 : $217.6 million  \n",
            "\n",
            "Extracted shape: (19482, 3)\n",
            "\n",
            "Extracted data saved to: extracted_columns.csv\n",
            "\n",
            "=== Extraction completed successfully! ===\n",
            "Total rows extracted: 19482\n"
          ]
        }
      ]
    }
  ]
}